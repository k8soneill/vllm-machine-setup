apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ vllm_deployment_name }}
  namespace: {{ vllm_namespace }}
spec:
  replicas: {{ vllm_replicas }}
  selector:
    matchLabels:
      app: {{ vllm_deployment_name }}
  template:
    metadata:
      labels:
        app: {{ vllm_deployment_name }}
    spec:
      strategy:
        type: Recreate
      runtimeClassName: nvidia
      containers:
      - name: vllm
        image: vllm/vllm-openai:latest
        args:
          - --model
          - {{ vllm_model }}
          - --quantization
          - {{ vllm_quantization }}
          - --dtype
          - {{ vllm_dtype }}
          - --max-model-len
          - "{{ vllm_max_model_len }}"
          - --gpu-memory-utilization
          - "{{ vllm_gpu_memory_utilization }}"
        env:
        - name: LD_LIBRARY_PATH
          value: /usr/lib/wsl/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
        - name: NVIDIA_VISIBLE_DEVICES
          value: all
        - name: NVIDIA_DRIVER_CAPABILITIES
          value: compute,utility
        ports:
        - containerPort: {{ vllm_port }}
          name: http
        resources:
          limits:
            nvidia.com/gpu: 1
        volumeMounts:
        - name: huggingface-cache
          mountPath: /root/.cache/huggingface
        - name: wsl-lib
          mountPath: /usr/lib/wsl/lib
          readOnly: true
      volumes:
      - name: huggingface-cache
        hostPath:
          path: {{ huggingface_cache }}
          type: DirectoryOrCreate
      - name: wsl-lib
        hostPath:
          path: /usr/lib/wsl/lib
---
apiVersion: v1
kind: Service
metadata:
  name: {{ vllm_service_name }}
  namespace: {{ vllm_namespace }}
spec:
  selector:
    app: {{ vllm_deployment_name }}
  ports:
  - port: {{ vllm_port }}
    targetPort: {{ vllm_port }}
  type: LoadBalancer
